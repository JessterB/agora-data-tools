# agora-data-tools

- [Intro](#intro)
- [Running the pipeline](#running-the-pipeline)
  - [GitHub-actions](#github-actions)
  - [Locally](#locally)
  - [Docker](#docker)
- [Testing Github Workflow](#testing-github-workflow)
- [Unit Tests](#unit-tests)
- [Config](#config)

## Intro
A place for Agora's ETL, data testing, and data analysis

This configuration-driven data pipeline uses a config file - which is easy for
engineers, analysts, and project managers to understand - to drive the entire ETL process.  The code in `/agoradatatools` uses
parameters defined in a config file to determine which kinds of extraction and transformations a particular
dataset needs to go through before the resulting data is serialized as json files that can be loaded into Agora's data repository.  

In the spirit of importing datasets with the minimum amount of transformations, one can simply add a dataset to the config file, 
and run the scripts. 

This `/agoradatatools` implementation was influenced by the "Modern Config Driven ELT Framework for Building a 
Data Lake" talk given at the Data + AI Summit of 2021.

Python notebooks that describe the custom logic for various datasets are located in `/data_analysis/notebooks`.

## Running the pipeline
The json files generated by `/agoradatatools` are written to folders in the [Agora Synapse project](https://www.synapse.org/#!Synapse:syn11850457/files/) by default, 
although you can modify the destination Synapse folder in the [config file](#Config).

Note that running the pipeline does _not_ automatically update the Agora database in any environment.  Ingestion of generated json files
into the Agora databases is handled by [agora-data-manager](https://github.com/Sage-Bionetworks/agora-data-manager/).

You can run the pipeline in any of the following ways:
1. [GitHub Actions](#GitHub Actions) is the simplest, but least flexible, way to run the pipeline; it does not require Synapse permissions, creating a Synpase PAT, or setting up the Synapse Python client.
2. [Locally](#Locally) requires installing Python, obtaining the required Synapse permissions, creating a Synpase PAT, and setting up the Synapse Python client.
3. [Docker](#Docker) requires installing Docker, obtaining the required Synapse permissions, and creating a Synpase PAT.

When running the pipeline, you must specify the config file that will be used. There are two config files that are checked into this repo:  
* ```test_config.yaml``` places the transformed datasets in the [Agora Testing Data](https://www.synapse.org/#!Synapse:syn17015333) folder in synapse; write files to this folder to perform data validation.
* ```config.yaml``` places the transformed datasets the [Agora Live Data](https://www.synapse.org/#!Synapse:syn12177492) synapse folder; write files to this folder once you've validated that the ETL process is generating files suitable for release. 
Note that files in the Agora Live Data folder are not automatically released, so if 'bad' file versions do get written to this folder it's not the end of the world. A releasable manifest file can be generated by a subsequent ETL processing run into the folder, or manually if necessary.

You may also create a custom config file to use locally to target specific dataset(s) or transforms of interest, and/or to write the generated json files to a different Synapse 
location. See the [config file](#Config) section for additional information.

### GitHub Actions
This pipeline can be executed without any local installation, permissions, or credentials; the repository is configured to use Agora's Synapse credentials, which can be found in LastPass in the "Shared-Agora" Folder. 

Use one of the following GitHub Actions to trigger the workflow:  
* The `CI` GitHub Action is automatically run any time that code is merged to the dev branch; this pipeline uses `test_config.yaml` so the generated json files are written to [Agora Testing Data](https://www.synapse.org/#!Synapse:syn17015333). This pipeline can also be manually triggered (for example, when a new source file version is available but there are no associated code changes to merge) by performing the following steps:

  1. Go to "Actions" Tab in this GitHub repository
  2. Click "CI" on the left
  3. Click "Run workflow" to update the files and manifest in the **_Agora Testing Data_** folder


* The `production-release` GitHub Action must be manually triggered; this pipeline uses ```config.yaml``` so the generated json files are written to [Agora Live Data](https://www.synapse.org/#!Synapse:syn12177492). To manually trigger the `production-release` pipeline perform the following steps:
  1. Go to "Actions" Tab in this GitHub repository
  2. Click "production_release" on the left
  3. Click "Run workflow" to update the files and manifest in the **_Agora Live Data_** folder

### Locally
Perform the following one-time steps to set up your local environment and to obtain the required Synapse permissions:

1. Due to the nature of Python, you will want to set up your python environment with [conda](https://www.anaconda.com/products/distribution) or [pyenv](https://github.com/pyenv/pyenv).  You will want to create a virtual environment to do your work.
    * conda - please follow instructions [here](https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html) to manage environments
    * pyenv - you will want to use [virtualenv](https://virtualenv.pypa.io/en/latest/) to manage your python environment

2. Install the package locally using conda or pyenv, depending on which you chose:

    * conda
      ```bash
      conda create -n agora python=3.9
      conda activate agora
      pip install .
      pip install -r requirements.txt
      ```
    * pyenv + virtualenv
      ```bash
      pyenv install -v 3.9.13
      pyenv global 3.9.13
      python -m venv env
      source env/bin/activate
      python3 -m pip install .
      python3 -m pip -r requirements.txt
      ```

3. Obtain download access to all required source files in Synapse, including accepting the terms of use on the AD Knowledge Portal backend [here](https://www.synapse.org/#!Synapse:syn5550378).  If you see a green unlocked lock icon, then you should be good to go.
4. Obtain write access to the destination Synapse project, e.g. [Agora Synapse project](https://www.synapse.org/#!Synapse:syn11850457/files/)
5. Create a Synapse personal access token (PAT)
6. [Set up](https://help.synapse.org/docs/Client-Configuration.1985446156.html) your Synapse Python client locally

Once you have completed the setup steps outlined above, execute the pipeline by running `process.py` and providing the desired [config file](#Config) as an argument. The following example command will execute the pipeline using ```test_config.yaml```:

    ```bash
    python ./agoradatatools/process.py test_config.yaml
    ```

### Docker

If you don't want to deal with Python paths and dependencies, you can use Docker to run the pipeline. Perform the following one-time steps to set up your docker environment and to obtain the required Synapse permissions:
1. Install [Docker](https://docs.docker.com/get-docker/).
2. Obtain download access to all required source files in Synapse, including accepting the terms of use on the AD Knowledge Portal backend [here](https://www.synapse.org/#!Synapse:syn5550378).  If you see a green unlocked lock icon, then you should be good to go.
3. Obtain write access to the destination Synapse project, e.g. [Agora Synapse project](https://www.synapse.org/#!Synapse:syn11850457/files/)
4. Create a Synapse personal access token (PAT)

Once you have completed the one-time setup steps outlined above, execute the pipeline by running the following command and providing your PAT and the desired [config file](#Config) as an argument. The following example command will execute the pipeline in Docker using ```test_config.yaml```:

```
# This creates a local docker image
docker build -t agora-data-pipeline .
docker run -e SYNAPSE_AUTH_TOKEN=<your PAT> agora-data-pipeline python ./agoradatatools/process.py test_config.yaml
```

## Testing Github Workflow
In order to test the GitHub Actions workflow locally:
- install [act](https://github.com/nektos/act) and (docker)[https://github.com/docker/docker-install]
- create a .secrets file in the root directory of the folder with a SYNAPSE_USER and a SYNAPSE_PASS value*

Then run:
```bash
act -v --secret-file .secrets
```

The repository is currently using Agora's credentials for Synapse.  Those can be found in LastPass in the "Shared-Agora" Folder.

## Unit Tests
Unit tests can be run by calling pytest from the command line.
```bash
python -m pytest
```

## Config
Parameters:
- `destination`: Defines the default target location (folder) that the generated json files are written to; this value can be overridden on a per-dataset basis
- `datasets/<dataset>`: Each generated json file is named `<dataset>.json`
- `datasets/<dataset>/files`: A list of source files for the dataset
    - `name`: The name of the source file (this name is the reference the code will use to retrieve a file from the configuration)
    - `id`: Synapse id of the file
    - `format`: The format of the source file
- `datasets/<dataset>/provenance`: The Synapse id of each entity that the dataset is derived from, used to populate the generated file's Synapse provenance. (The Synapse API calls this "Activity")
- `datasets/<dataset>/destination`: Override the default destination for a specific dataset by specifying a synID, or use `*dest` to use the default destination
- `datasets/<dataset>/column_rename`: Columns to be renamed prior to data transformation
- `datasets/<dataset>/agora_rename`: Columns to be renamed after data transformation, but prior to json serialization
- `datasets/<dataset>/custom_transformations`: The list of additional transformations to apply to the dataset; a value of 1 indicates the default transformation
